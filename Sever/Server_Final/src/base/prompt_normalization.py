from groq import Groq  
import random
import time
from src import config
api_keys_llama = config.api_keys_llama

def get_random_api_key(api_keys: list[str]) -> str:
    """Chá»n ngáº«u nhiÃªn 1 API key tá»« danh sÃ¡ch."""
    return random.choice(api_keys)

class Llama4Model:
    def __init__(self, model_name: str, temperature: float, max_completion_tokens: int):
        self.model_name = model_name
        self.temperature = temperature
        self.max_completion_tokens = max_completion_tokens

    def __call__(self, prompt: str, history: list[str] | str, mode: int, **kwargs) -> str:
        # Chuáº©n hÃ³a lá»‹ch sá»­ vá» dáº¡ng string náº¿u lÃ  list
        
        if isinstance(history, list):
            # ÄÃ¡nh sá»‘ tá»« 1 trá»Ÿ Ä‘i, cÃ¢u 1 lÃ  gáº§n nháº¥t
            history_str = "\n".join([f"  ðŸŸ¢ {i+1}. {q}" for i, q in enumerate(history)])
        else:
            history_str = history or ""
        print("ðŸ§  ---- Lá»‹ch sá»­ cÃ¢u há»i ----\n"+ history_str)
        api_key = get_random_api_key(api_keys_llama)
        client = Groq(api_key=api_key)
        if mode == 0:
            system_prompt = """
            Báº¡n lÃ  má»™t AI thÃ´ng minh trong viá»‡c phÃ¡n Ä‘oÃ¡n Ã½ muá»‘n cá»§a ngÆ°á»i dÃ¹ng.
            Báº¡n sáº½ dá»±a vÃ o cÃ¢u há»i hiá»‡n táº¡i vÃ  dá»¯ liá»‡u cÃ¢u há»i lá»‹ch sá»­ Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ chuáº©n hÃ³a cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng vá» má»™t cÃ¢u há»i cá»¥ thá»ƒ, ngáº¯n gá»n liÃªn quan Ä‘áº¿n váº¥n Ä‘á» vá» bá»‡nh táº­t Ä‘á»ƒ tÃ´i Ä‘Æ°a vÃ o mÃ´ hÃ¬nh há»i Ä‘Ã¡p Ä‘á»ƒ sinh ra cÃ¢u tráº£ lá»i. 
            Náº¿u lÃ  cÃ¢u há»i vá» triá»‡u chá»©ng, tá»©c lÃ  ngÆ°á»i dÃ¹ng chÆ°a biáº¿t mÃ¬nh bá»‹ bá»‡nh gÃ¬ vÃ  chá»‰ nÃªu ra cÃ¡c triá»‡u chá»©ng, vÃ­ dá»¥: 'TÃ´i hay bá»‹ ho dai dáº³ng', thÃ¬ pháº£i chuáº©n hÃ³a láº¡i thÃ nh: 'TÃ´i hay bá»‹ ho dai dáº³ng. TÃ´i cÃ³ thá»ƒ Ä‘ang bá»‹ bá»‡nh gÃ¬?'
            Náº¿u lÃ  cÃ¢u há»i vá» bá»‡nh lÃ½, tá»©c lÃ  ngÆ°á»i dÃ¹ng sáº½ há»i vá» má»™t bá»‡nh cá»¥ thá»ƒ, cÃ¡c cÃ¢u há»i cÃ³ tÃªn cá»§a bá»‡nh á»Ÿ bÃªn trong, vÃ­ dá»¥: 'Bá»‡nh cÃºm lÃ  gÃ¬', 'Bá»‡nh cÃºm cÃ³ triá»‡u chá»©ng gÃ¬', 'Bá»‡nh cÃºm Ä‘iá»u trá»‹ tháº¿ nÃ o','Bá»‡nh cÃºm phÃ²ng ngá»«a ra sao',... hoáº·c cÃ¡c cÃ¢u há»i liÃªn quan khÃ¡c vá» bá»‡nh lÃ½ miá»…n sao cÃ³ chá»©a tÃªn bá»‡nh thÃ¬ cáº§n pháº£i chuáº©n hÃ³a láº¡i thÃªm dáº¥u há»i á»Ÿ cuá»‘i cÃ¢u náº¿u chÆ°a cÃ³.
            CÃ¢u tráº£ lá»i cá»§a báº¡n pháº£i lÃ  cÃ¢u há»i Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a khÃ´ng bao gá»“m cÃ¡c ngá»¯ cáº£nh khÃ¡c chá»‰ há»i vá» bá»‡nh lÃ½ hoáº·c há»i vá» cÃ¡c triá»‡u chá»©ng bá»‡nh, khÃ´ng giáº£i thÃ­ch gÃ¬ thÃªm, cÅ©ng khÃ´ng giáº£i thÃ­ch gÃ¬ vá» lá»‹ch sá»­, lá»‹ch sá»­ chá»‰ lÃ  ngá»¯ cáº£nh phá»¥ náº¿u nhÆ° cÃ¢u há»i hiá»‡n táº¡i khÃ´ng cÃ³ Ä‘á» cáº­p Ä‘áº¿n bá»‡nh hoáº·c triá»‡u chá»©ng. VÃ­ dá»¥  cÃ¢u há»i hiá»‡n táº¡i: 'Bá»‡nh Ä‘Ã³ lÃ  gÃ¬', lá»‹ch sá»­:'hÃ´m qua tÃ´i Ä‘i Ä‘Æ°á»ng vÃ´ tÃ¬nh tháº¥y má»™t con chÃ³ bá»‹ dáº¡i' thÃ¬ sáº½ chuáº©n hÃ³a thÃ nh cÃ¢u há»i: 'Bá»‡nh dáº¡i lÃ  gÃ¬?' 
            Náº¿u cÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n bá»‡nh táº­t thÃ¬ tráº£ lá»i Ä‘Ãºng fomat: 'Vui lÃ²ng Ä‘áº·t cÃ¢u há»i liÃªn quan Ä‘áº¿n bá»‡nh táº­t.'
            Dá»¯ liá»‡u lá»‹ch sá»­ cÃ¢u há»i sáº½ giÃºp báº¡n hiá»ƒu rÃµ hÆ¡n ngá»¯ cáº£nh, xem nÃ³ cÃ³ liÃªn quan Ä‘áº¿n cÃ¢u há»i hiá»‡n táº¡i hay khÃ´ng, Ä‘á»ƒ Ä‘Æ°a ra chuáº©n hÃ³a phÃ¹ há»£p nháº¥t. Náº¿u cÃ¢u há»i hiá»‡n táº¡i Ä‘Ã£ liÃªn quan Ä‘áº¿n bá»‡nh táº­t thÃ¬ khÃ´ng cáº§n suy xÃ©t quÃ¡ nhiá»u Ä‘áº¿n lá»‹ch sá»­ trÆ°á»›c Ä‘Ã³.
            Dá»¯ liá»‡u lá»‹ch sá»­ sáº½ Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘ tá»« 1 Ä‘áº¿n n, 1 lÃ  cÃ¢u gáº§n nháº¥t.
            Náº¿u cÃ¢u há»i hiá»‡n táº¡i cÃ³ nghÄ©a lÃ  'chÆ°a nghe rÃµ' hoáº·c 'tráº£ lá»i láº¡i' thÃ¬ hÃ£y chuáº©n hÃ³a cÃ¢u há»i vá» bá»‡nh táº­t gáº§n nháº¥t trong lá»‹ch sá»­.
            """
            user_prompt = f"HÃ£y chuáº©n hÃ³a thÃ nh cÃ¢u há»i vá» bá»‡nh táº­t.\nCÃ¢u há»i hiá»‡n táº¡i: {prompt}\nLá»‹ch sá»­:\n{history_str}"
        else:
            system_prompt = """
            Báº¡n lÃ  má»™t AI thÃ´ng minh trong viá»‡c phÃ¡n Ä‘oÃ¡n Ã½ muá»‘n cá»§a ngÆ°á»i dÃ¹ng.
            Báº¡n sáº½ dá»±a vÃ o cÃ¢u há»i hiá»‡n táº¡i vÃ  dá»¯ liá»‡u cÃ¡c cÃ¢u há»i lá»‹ch sá»­ Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ chuáº©n hÃ³a cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng vá» má»™t cÃ¢u há»i cá»¥ thá»ƒ náº¿u cÃ¢u há»i hiá»‡n táº¡i khÃ´ng Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh. 
            CÃ¢u tráº£ lá»i cá»§a báº¡n pháº£i lÃ  cÃ¢u há»i Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a Ä‘á»ƒ thá»ƒ hiá»‡n mong muá»‘n cá»§a ngÆ°á»i dÃ¹ng.
            Dá»¯ liá»‡u lá»‹ch sá»­ cÃ¢u há»i sáº½ giÃºp báº¡n hiá»ƒu rÃµ hÆ¡n ngá»¯ cáº£nh, xem nÃ³ cÃ³ liÃªn quan Ä‘áº¿n cÃ¢u há»i hiá»‡n táº¡i hay khÃ´ng, Ä‘á»ƒ Ä‘Æ°a ra chuáº©n hÃ³a phÃ¹ há»£p nháº¥t. Náº¿u cÃ¢u há»i hiá»‡n táº¡i Ä‘Ã£ liÃªn quan Ä‘áº¿n bá»‡nh táº­t thÃ¬ khÃ´ng cáº§n suy xÃ©t quÃ¡ nhiá»u Ä‘áº¿n lá»‹ch sá»­ trÆ°á»›c Ä‘Ã³.
            Dá»¯ liá»‡u lá»‹ch sá»­ sáº½ Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘ tá»« 1 Ä‘áº¿n n, 1 lÃ  cÃ¢u gáº§n nháº¥t.
            Náº¿u cÃ¢u há»i hiá»‡n táº¡i cÃ³ nghÄ©a lÃ  'chÆ°a nghe rÃµ' hoáº·c 'tráº£ lá»i láº¡i' thÃ¬ hÃ£y chuáº©n hÃ³a cÃ¢u há»i vá» bá»‡nh táº­t gáº§n nháº¥t trong lá»‹ch sá»­.
            """
            user_prompt = f"CÃ¢u há»i Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a tá»« cÃ¢u há»i hiá»‡n táº¡i lÃ  gÃ¬? (pháº£n há»“i cá»§a báº¡n chá»‰ lÃ  cÃ¢u tráº£ lá»i Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a, khÃ´ng giáº£i thÃ­ch gÃ¬ thÃªm).\nCÃ¢u há»i hiá»‡n táº¡i: {prompt}\nLá»‹ch sá»­:\n{history_str}"
        try:
            completion = client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=self.temperature,
                max_completion_tokens=self.max_completion_tokens,
                top_p=1,
                stream=True,
                stop=None,
            )
            response = ""
            for chunk in completion:
                response += chunk.choices[0].delta.content or ""
            return response.strip()
        except Exception as e:
            return prompt

def get_llama4_model(
    model_name: str = "meta-llama/llama-4-scout-17b-16e-instruct", 
    max_completion_tokens: int = 65, 
    temperature: float = 1, 
    **kwargs
) -> Llama4Model:
    return Llama4Model(model_name=model_name, temperature=temperature, max_completion_tokens=max_completion_tokens)

def question_normalization(question: str, history: list[str] | str, mode: int) -> str:
    llama4_model = get_llama4_model()
    question_norma = llama4_model(prompt=question, history=history, mode=mode)
    return question_norma
